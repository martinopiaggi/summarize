{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObS8qFDIYdpMAvt48k6pwd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinopiaggi/summarize/blob/main/summarize_videos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using https://huggingface.co/facebook/bart-large-cnn"
      ],
      "metadata": {
        "id": "4QOu6v4AuX21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow==2.1\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "f_OFp3uMzB9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bf4601-679b-48cd-dee6-8736b93b0991"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.1 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vK-jCVHOtE1g"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown Just paste the text here:\n",
        "DOCUMENT = \"00:00:00 In his State of the Union Address, President Biden spoke about the Russian aggression in Ukraine and the need for the free world to stand united against it. He also thanked the Ukrainian people for their courage and determination in standing up to Russia. 00:05:00 In his State of the Union address, President Biden announced that the United States would be increasing sanctions against Russia and working with other countries to isolate Putin. He also announced that the Department of Justice would be assembling a task force to go after the crimes of Russian oligarchs. 00:10:00 In his State of the Union Address, President Biden discusses the progress made in combating the COVID-19 pandemic and the American Rescue Plan. He also talks about the need for continued investment in the economy to create jobs and improve the standard of living for working Americans. 00:15:00 In his State of the Union address, President Biden announced a number of initiatives to improve infrastructure and spur economic growth. He also called for increased investment in research and development, and for the passage of a bipartisan innovation act. 00:20:00 President Biden's State of the Union Address focused on the importance of technology and innovation in America. He announced that intel is investing 100 billion dollars in computer chips, and that 369 000 new manufacturing jobs were created in America last year. He also mentioned that his top priority is getting prices under control. 00:25:00 In his State of the Union Address, President Biden proposed a plan to lower the cost of prescription drugs, child care, and energy, as well as increase investment in clean energy and affordable housing. He argued that these measures would improve the standard of living for hardworking Americans and boost the economy. 00:30:00 In his State of the Union address, President Biden proposed raising taxes on corporations and the wealthy to help fund programs for the middle class and small businesses. He also announced a crackdown on companies that overcharge consumers and a plan to improve the quality of nursing homes. 00:35:00 In his State of the Union address, President Biden discussed the progress made in the fight against COVID-19 and the steps that need to be taken to continue moving forward safely. He announced new initiatives to provide more testing and treatment options for Americans, and called on Congress to provide the necessary funding to support these efforts. 00:40:00 President Biden gives his State of the Union Address, discussing the progress made in the fight against COVID-19 and the need to continue vaccinating the world. He also talks about the need to restore trust in law enforcement and to invest in proven strategies to reduce crime. 00:45:00 In his State of the Union address, President Biden called on Congress to pass legislation to reduce gun violence, expand voting rights, and reform the immigration system. He also honored retiring Supreme Court Justice Stephen Breyer. 00:50:00 President Biden's State of the Union Address included a call for unity and a focus on four areas: beating the opioid epidemic, taking on mental health, supporting veterans, and advancing liberty and justice. He also spoke out against state laws targeting transgender Americans and called for the passage of the Equality Act. 00:55:00 President Biden gives his State of the Union Address, focusing on the importance of unity in addressing the nation's challenges. He discusses his commitment to finding out more about the health effects of burn pits on soldiers, expanding benefits for veterans, and funding research to end cancer.\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)\n",
        "DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
        "DOCUMENT = DOCUMENT.strip()"
      ],
      "metadata": {
        "id": "pq8J8QOQtVX5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\",device=0)\n"
      ],
      "metadata": {
        "id": "Gjzei7H101vt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "tokenizer = summarizer.tokenizer\n",
        "tokens = tokenizer.encode(DOCUMENT)\n",
        "\n",
        "print(len(tokens))\n",
        "\n",
        "\n",
        "# Calculate the number of chunks needed\n",
        "chunk_len = math.ceil(len(tokens) / 1024)\n",
        "chunksNumber = len(tokens)//chunk_len\n",
        "\n",
        "# Split the tokens into chunks\n",
        "chunks = [tokens[i:i+chunksNumber] for i in range(0, len(tokens), chunksNumber)]\n",
        "\n",
        "\n",
        "for chunk in chunks:\n",
        "\n",
        "    # Set max_length and min_length based on token count\n",
        "    max_length = len(chunk) // 2\n",
        "    min_length = len(chunk) // 5\n",
        "\n",
        "    chunkText = tokenizer.decode(chunk)\n",
        "\n",
        "\n",
        "    # Generate summary for each chunk without sampling (example)\n",
        "    summary = summarizer(chunkText, max_length=max_length, min_length=min_length, do_sample=True)\n",
        "\n",
        "    print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NnXQzbOE_e5",
        "outputId": "ae3fa78b-af0e-4677-dffc-626571faee94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10998\n",
            "[{'summary_text': 'A vector clock defines a perfect isomorphism with respect to the happens before relationship. When an event happens at a process, it increments its own position. The event of sending messages is an event. So the vector clock of p1 becomes 2, 0, 0. Now p2 receives the message and the message is sent. The vector clock is put on top of this application message. At the end, this is the result of a clock that represents the vision of a process or regarding the other processes. So each time you send a message, you send it to the right place. You send the message to process two. At process two, the clock was 0,0,0. Now process two received something, so we increment the second position of its own vector clock. The first position of process one represents the view of how many events happen at one of the processes. And if you look at the clock of A and the algorithm, they are this one act as the result.'}]\n",
            "[{'summary_text': \"Using vector clock, we can order events according to causality, not exactly causality. In order to do so, we need a variation of vector clock. For example, imagine that we have a group communication system. We want to order questions and replies, messages in causal order, not in a total order. Using vector clock we can do this using a strong ordering, which we use in which we increment, in which two things happen in two places. We don't need to order messages that are connected by an offense before relationship, but those that are in an ordering with the vector clock ordering. The two messages are parallel, so the ordering is not relevant for us. So it tries and succeeds in ordering messages, even if they are not parallel. Tries to order broadcast, the totally ordered broadcast, tries to order them in a certain certain order, and then then the second or vice versa for all of us. We may accept that one of us here are the first and then the other of us are the second, depending on the speed of the channel.\"}]\n",
            "[{'summary_text': \"The Hypothesis are that messages and replies are sent using reliable Five for the channel. The idea is that you must There is this communication in which this is a broadcast communication which all messages are sent to everyone. This means that if this channel would not be FICO ordering, only two, p0 sends a message and then sends another message. There are two messages here we have 1, 0 and here we here we we have 2, 0. The two messages are enough as they have been sent by the same process. Now, at the position of the sender, my view is lower per se per se, which is fine. He knows better than me what happens at se. I could accept it in principle, but I don't accept it because I could. accept it simply because I donâ€™t accept that his own view is greater than mine. So, which of these hypothesis is too strong? So the fact that the messages were sent to all the boards in parallel We just commented is that required Hypotheses are too strong.\"}]\n",
            "[{'summary_text': 'Man speaks off mic: \"There\\'s no ordering, no ordering is important\" Man: \"When I update my, if I have a long queue, I take all the messages that are in the queue, and all of them that satisfy that condition are sent to the application\" \"This is the worst possible situation, possible. This is the speed free process,\" he adds. \"This guarantees that I can deliver in any order this guarantees that their relationship I can take from the queue and in fact I take from any position and then look at the usual.\" \"There is no way to predict how long it will take to send a message,\" he says. \"I don\\'t know how much time it takes to send and receive a message. I don\\'t even know if I can predict the amount of time that it takes for a message to be sent and received\" \"It\\'s a miracle that we have a computer that works this way,\" says the man. \"It is a miracle we can do this.\"'}]\n",
            "[{'summary_text': \"Mutual exclusion is required to prevent interference between processes in a distributed system. The idea is that there is a single process that controls the access of the printer. The safety properties says that at most one process exits the critical section at the top. The critical section, in our example, is the print. processes are reliable, of course, in place. Now, you go access the printer directly, print, print all your documents, all your 100 pages. Then you come back, you give me back the token, the token that gives you the right to print. And then I know that the printer is free, and I can give the token to the second process, and so on so forth. This is the idea of having a centralized process that It is not printing, it does not hold the resource, so it's not printing. And it was not interested in printing, and it did not send a message requesting to access the printers before, so. it is notprinting. Now if the process holds the request, it puts the request in a local queue without reply, according to the timestamp of the request.\"}]\n",
            "[{'summary_text': \"Each process resolves the ties, the competition locally by looking at the lamp or clock of its own request and the lamp of the other request that it received. Only one, at most one process go printing at each time. Sooner or later, you will get acknowledged. And this is guaranteed by the fact that there is always a total order between the numbers. The liveness property is also guaranteed, because sooner or later you will receive the right to print, unless there is a crash that takes the printer forever. So now I have a question for you, the optional condition for you is the fairness is satisfied? So AppS4.4 could be given in order? No, because it could be in any order, so it could also be given order in order. So, the assumption is that if the process is given the right. to access the critical section, sooner or. later it doesn't crash or stops forever into the critical. section. This does not require any centralized process. Second approach, we are all together. We want like to access this printer without printing in parallel. We organize ourselves in a ring. You can say I, precedes you, then precede you, and this ring. It's not a physical ring.\"}]\n",
            "[{'summary_text': \"Lamport is an open source system that lets users control printers on the Internet. LEMPORT is a type of distributed network with a number of different types of printers. In the best possible situation, only two messages are required to print a token. But the overhead can be potentially infinite in order to access the print function. Lemport is based on the idea of a leader, a single process that coordinates the action of other processes, and a color cloth that can be used to represent the color of the color cloth. For a centralized solution, if the coordinator crash, we have a problem. But for the lamp work solution,  if any process crash, We have a Problem. And for the token ring solution, for the Token Ring solution, If any process crashes, We Have A Problem. We assume that the system is closed. So we assume that all processes knows the group within with the agreement must happen. So the duration of the protocol remains stable for each group. If there are crashes, we want that no processes enter the group. Processes entering the group may crash for election, because it's exactly the situation that we want to call it.\"}]\n",
            "[{'summary_text': \"We assume links are reliable, and we assume that we can see the crash, which is a strong assumption. Because understanding that someone crashed means that we are in a synchronous system. That's the only way to distinguish between a very long channel and a crashed process. We can realize we know each other, we can realize that the leader has crashed and we have to elect a new leader. First protocol, bully election. The other approach is, again, using a ring. Ring is a structure as you discovered, probably understood during the course. In order to implement proper rules, we may also use a logical ring. We assume a process detects a failure of the leader, it sends an elect message containing its own ID to the closest neighbor that is still available. At the same time, two and five discovered that seven crashed, so two sends an election message to three. Three circulates the message adding its ownID to three with its own message. The bully wins. The process with the highest ID that remained after the crash wins.\"}]\n",
            "[{'summary_text': \"There is first an elect phase and then there is a coordinate phase, exactly like in this case an election and then a coordination phase. Once the process receives a message that contains its own ID, the process takes the list of IDs, takes the greatest one or the lowest one, or the one that you want. The only important thing is that everyone choose the same. The system proceeds this way with those two tokens that circulate in parallel. parallel. It would like to take a picture of the system, and what you would like is a picture like the one we can obtain here. So what we can do here is to observe the entire system in zero time, so have a perfect view of the whole system. But God does not exist, so this picture may not exist. We can compare the two algorithms, I'll leave this to you. Global state, collecting global state, and then termination detection. Capturing global state is a problem that one application of this case is the problem that we already encountered, the problem of creating a snapshot of a system.\"}]\n",
            "[{'summary_text': \"Each process is pictured at different times. A cut is consistent if for every two events, e and f, where e is part of the cut and f happens before e, this is the happens before relationship. Atomically, but this is something that happens on a single process, so this can be done atomically. So when a process starts the snapshot, it takes its own state and emits a special message on all outgoing channels that it has done something. So we are in a situation in which processes are doing something like the banks before the banks. They are connected by channels that communicate by the various action channels, so we will see each channel with its own direction. Some of them will be by action or not, so the marker of the marker, the marker on the outgoing channels, is the marker for the action channel. This is the protocol of the protocol, the token of the token, the markers of the markers, the tokens of the tokens. It's called the Message Queuing Protocol (MQP)\"}]\n",
            "[{'summary_text': \"Theory: The distributed snapshot algorithm selects a consistent cut. EI and EJA are two events that occur the first at process high and the second at process J. And those two events are such that EI happens before EJA. If EI appears after EI, we say this state, then EI is sent to market before M1 and MH. So EI appeared after the market sent its state, so E0 occurred after P0 saved its state. So there is first the sending of the market, and then there is the event EI. So if E i is not part of the cut, then this means that E i occurred after E i. E j is an event that occurred at P j, and E J is part of. the snapshot. So this means E j occurred before P j saved its. state, right? In order to be part. of the saving, it must happen before the saving. If it happens after the saving,. it's gone. If there is no communication, the two. events are parallel, okay? So EJ at the receiver of the last message. This is the only condition by which EI happening before EJ.\"}]\n",
            "[{'summary_text': ''}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "chunkLen = math.ceil(11755/1024)\n",
        "\n",
        "\n",
        "\n",
        "chunks = [str[i:i+chunkLen] for i in range(0, len(str), chunkLen)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OgYmBGAz0ML",
        "outputId": "c831770a-f7de-4fd3-ec16-694d22514ae9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc8hsC6e1HLH",
        "outputId": "8101d993-af1a-4782-8f68-2199929096e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrR1_IBS-PYs",
        "outputId": "4a2e0514-6ff1-483c-df7a-8403df7f5c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-37b_fvk3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-37b_fvk3\n",
            "  Resolved https://github.com/openai/whisper.git to commit b38a1f20f4b23f3f3099af2c3e0ca95627276ddf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230918) (10.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230918)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230918) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.27.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (3.12.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230918) (17.0.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230918) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230918) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230918) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230918) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230918) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230918-py3-none-any.whl size=798405 sha256=c99b4370521887be1ff1da77fc9d2fee23a00e283d7d03c56dd6675a9fc515bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wkv9w7zj/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230918 tiktoken-0.3.3\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.10.13-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-11.0.3 yt-dlp-2023.10.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** ğŸ§ \n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'base.en' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "BSfDK-q5-Ybi",
        "outputId": "1f096fd1-c127-4a0b-92f3-3d90e431fbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:01<00:00, 100MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**base.en model is selected.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** ğŸ“º\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/L_Guz73e6fw\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    'outtmpl': '%(id)s.%(ext)s',\n",
        "    # â„¹ï¸ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "    'postprocessors': [{  # Extract audio using ffmpeg\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }]\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download([URL])\n",
        "    list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "for video_info in list_video_info:\n",
        "    video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4co29Oz9_AXJ",
        "outputId": "ae7df905-0179-4543-b691-c0ec7f25e54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/L_Guz73e6fw\n",
            "[youtube] L_Guz73e6fw: Downloading webpage\n",
            "[youtube] L_Guz73e6fw: Downloading ios player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading android player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading m3u8 information\n",
            "[info] L_Guz73e6fw: Downloading 1 format(s): 140\n",
            "[download] Destination: L_Guz73e6fw.m4a\n",
            "[download] 100% of  133.30MiB in 00:00:03 at 41.10MiB/s  \n",
            "[FixupM4a] Correcting container of \"L_Guz73e6fw.m4a\"\n",
            "[ExtractAudio] Destination: L_Guz73e6fw.wav\n",
            "Deleting original file L_Guz73e6fw.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://youtu.be/L_Guz73e6fw\n",
            "[youtube] L_Guz73e6fw: Downloading webpage\n",
            "[youtube] L_Guz73e6fw: Downloading ios player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading android player API JSON\n",
            "[youtube] L_Guz73e6fw: Downloading m3u8 information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Run the model** ğŸš€\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** âš™ï¸\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"English\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'all' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Optional: Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = whisper.transcribe(\n",
        "        whisper_model,\n",
        "        str(video_path_local),\n",
        "        temperature=temperature,\n",
        "        **args,\n",
        "    )\n",
        "\n",
        "    # Save output\n",
        "    whisper.utils.get_writer(\n",
        "        output_format=output_format,\n",
        "        output_dir=video_path_local.parent\n",
        "    )(\n",
        "        video_transcription,\n",
        "        str(video_path_local.stem),\n",
        "        options=dict(\n",
        "            highlight_words=False,\n",
        "            max_line_count=None,\n",
        "            max_line_width=None,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def exportTranscriptFile(ext: str):\n",
        "        local_path = video_path_local.parent / video_path_local.with_suffix(ext)\n",
        "        export_path = drive_whisper_path / video_path_local.with_suffix(ext)\n",
        "        shutil.copy(\n",
        "            local_path,\n",
        "            export_path\n",
        "        )\n",
        "        display(Markdown(f\"**Transcript file created: {export_path}**\"))\n",
        "\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('.txt', '.vtt', '.srt', '.tsv', '.json'):\n",
        "            exportTranscriptFile(ext)\n",
        "    else:\n",
        "        exportTranscriptFile(\".\" + output_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iOTFm1vPAVDh",
        "outputId": "e09d9c2c-0837-40e6-f8b4-08e742e226c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### L_Guz73e6fw.wav"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:03.840]  We have been a misunderstood and badly mocked org for a long time.\n",
            "[00:03.840 --> 00:09.600]  Like when we started, we announced the org at the end of 2015,\n",
            "[00:10.800 --> 00:12.320]  and said we were going to work on AGI.\n",
            "[00:12.320 --> 00:14.320]  Like people thought we were batshit insane.\n",
            "[00:16.720 --> 00:23.760]  I remember at the time, a eminent AI scientist at a large industrial AI lab\n",
            "[00:24.320 --> 00:29.600]  was like DMing individual reporters being like, these people aren't very good,\n",
            "[00:29.600 --> 00:33.040]  and it's ridiculous to talk about AGI, and I can't believe you're giving them time of day,\n",
            "[00:33.040 --> 00:38.160]  and it's like that was the level of like pettiness and rancor in the field that a new group of people\n",
            "[00:38.160 --> 00:43.600]  say we're going to try to build AGI. So open AI and DeepMind was a small collection of folks\n",
            "[00:43.600 --> 00:52.160]  who were brave enough to talk about AGI in the face of mockery. We don't get mocked as much now.\n",
            "[00:53.040 --> 00:54.400]  Don't get mocked as much now.\n",
            "[00:56.720 --> 01:03.920]  The following is a conversation with Sam Altman, CEO of OpenAI, the company behind GPT-4,\n",
            "[01:03.920 --> 01:10.960]  JAD GPT, Dolly, Codex, and many other AI technologies, which both individually and together\n",
            "[01:10.960 --> 01:15.520]  constitute some of the greatest breakthroughs in the history of artificial intelligence,\n",
            "[01:15.520 --> 01:21.760]  computing, and humanity in general. Please allow me to say a few words about the possibilities\n",
            "[01:22.160 --> 01:26.720]  and the dangers of AI in this current moment in the history of human civilization.\n",
            "[01:27.360 --> 01:33.280]  I believe it is a critical moment. We stand on the precipice of fundamental societal transformation,\n",
            "[01:33.280 --> 01:38.640]  where soon nobody knows when, but many including me believe it's within our lifetime.\n",
            "[01:39.280 --> 01:45.680]  The collective intelligence of the human species begins to pale in comparison by many orders of\n",
            "[01:45.760 --> 01:53.840]  magnitude to the general superintelligence in the AI systems we build and deploy at scale.\n",
            "[01:55.200 --> 02:02.320]  This is both exciting and terrifying. It is exciting because of the innumerable applications\n",
            "[02:02.320 --> 02:09.360]  we know and don't yet know that will empower humans to create, to flourish, to escape the\n",
            "[02:09.360 --> 02:16.160]  widespread poverty and suffering that exists in the world today and to succeed in that old,\n",
            "[02:16.160 --> 02:23.440]  all-too-human pursuit of happiness. It is terrifying because of the power that super-intelligent\n",
            "[02:23.440 --> 02:30.960]  AGI wields to destroy human civilization, intentionally or unintentionally. The power\n",
            "[02:30.960 --> 02:39.040]  to suffocate the human spirit in the totalitarian way of George Orwell's 1984 or the pleasure-fueled\n",
            "[02:39.040 --> 02:46.240]  mass hysteria of Brave New World, where as Huxley saw it, people come to love their oppression,\n",
            "[02:46.800 --> 02:55.200]  to adore the technologies that undo their capacities to think. That is why these conversations\n",
            "[02:55.200 --> 03:01.200]  with the leaders, engineers and philosophers, both optimists and cynics, is important now.\n",
            "[03:02.800 --> 03:08.240]  These are not merely technical conversations about AI. These are conversations about power,\n",
            "[03:08.240 --> 03:13.280]  about companies, institutions and political systems that deploy, check and balance this power,\n",
            "[03:14.000 --> 03:20.480]  about distributed economic systems that incentivize the safety and human alignment of this power,\n",
            "[03:21.200 --> 03:27.520]  about the psychology of the engineers and leaders that deploy AGI, and about the history of human\n",
            "[03:27.520 --> 03:37.360]  nature, our capacity for good and evil at scale. I am deeply honored to have gotten to know and\n",
            "[03:37.360 --> 03:43.040]  to have spoken with on and off the mic with many folks who now work at OpenAI, including\n",
            "[03:43.040 --> 03:52.320]  Sam Altman, Greg Brockman, Iliya Sitzgever, Wojczyk, Saramba, Andrei Karpathy, Jakob Pachaki and many\n",
            "[03:52.320 --> 03:58.560]  others. It means the world that Sam has been totally open with me, willing to have multiple\n",
            "[03:58.560 --> 04:04.400]  conversations, including challenging ones, on and off the mic. I will continue to have these\n",
            "[04:04.480 --> 04:10.720]  conversations to both celebrate the incredible accomplishments of the AI community and the\n",
            "[04:10.720 --> 04:16.000]  steel man, the critical perspective on major decisions various companies and leaders make,\n",
            "[04:16.640 --> 04:24.400]  always with the goal of trying to help in my small way. If I fail, I will work hard to improve.\n",
            "[04:25.120 --> 04:31.280]  I love you all. This is the Lex Friedman podcast. To support it, please check out our sponsors in\n",
            "[04:31.280 --> 04:35.520]  the description. And now, dear friends, here's Sam Altman.\n",
            "[04:36.880 --> 04:42.480]  Hi Level. What is GPT for? How does it work and what to use most amazing about it?\n",
            "[04:43.120 --> 04:50.320]  It's a system that we'll look back at and say it was a very early AI, and it's slow, it's buggy,\n",
            "[04:50.960 --> 04:55.440]  it doesn't do a lot of things very well, but neither did the very earliest computers,\n",
            "[04:56.080 --> 05:01.840]  and they still pointed a path to something that was going to be really important in our lives,\n",
            "[05:01.840 --> 05:06.960]  even though it took a few decades to evolve. Do you think this is a pivotal moment? Out of\n",
            "[05:06.960 --> 05:13.200]  all the versions of GPT 50 years from now, when they look back at an early system that was really\n",
            "[05:13.200 --> 05:19.200]  kind of a leap in a Wikipedia page about the history of artificial intelligence, which of the\n",
            "[05:19.200 --> 05:24.080]  GPTs would they put? That is a good question. I sort of think of progress as this continual\n",
            "[05:24.160 --> 05:30.160]  exponential. It's not like we could say here was the moment where AI went from not happening to\n",
            "[05:30.160 --> 05:35.920]  happening, and I'd have a very hard time pinpointing a single thing. I think it's this very continual\n",
            "[05:35.920 --> 05:40.400]  curve. Well, the history books write about GPT one or two or three or four or seven.\n",
            "[05:41.280 --> 05:46.560]  That's for them to decide. I don't really know. I think if I had to pick some moment\n",
            "[05:47.200 --> 05:52.880]  from what we've seen so far, I'd sort of pick chat GPT. It wasn't the underlying model that\n",
            "[05:52.880 --> 05:56.560]  mattered. It was the usability of it, both the RLHF and the interface to it.\n",
            "[05:57.600 --> 06:03.520]  What is chat GPT? What is RLHF reinforcement learning with human feedback? What was that\n",
            "[06:03.520 --> 06:09.680]  little magic ingredient to the dish that made it so much more delicious?\n",
            "[06:10.560 --> 06:17.200]  So we train these models on a lot of text data, and in that process, they learn the underlying\n",
            "[06:18.080 --> 06:24.080]  something about the underlying representations of what's in here or in there. They can do\n",
            "[06:25.040 --> 06:29.840]  amazing things. But when you first play with that base model that we call it after you finish\n",
            "[06:29.840 --> 06:35.840]  training, it can do very well on evals. It can pass tests. It can do a lot of knowledge in there.\n",
            "[06:36.480 --> 06:43.840]  But it's not very useful, or at least it's not easy to use, let's say. And RLHF is how we take\n",
            "[06:43.840 --> 06:49.600]  some human feedback. The simplest version of this is show two outputs, ask which one is better\n",
            "[06:49.600 --> 06:54.720]  than the other, which one the human raiders prefer, and then feed that back into the model\n",
            "[06:54.720 --> 07:00.320]  with reinforcement learning. And that process works remarkably well, with in my opinion,\n",
            "[07:00.320 --> 07:07.440]  remarkably little data, to make the model more useful. So RLHF is how we align the model to\n",
            "[07:07.520 --> 07:14.080]  what humans want it to do. So there's a giant language model that's trained in a giant data set\n",
            "[07:14.080 --> 07:18.080]  to create this kind of background wisdom knowledge that's contained within the internet.\n",
            "[07:19.280 --> 07:25.840]  And then somehow adding a little bit of human guidance on top of it through this process\n",
            "[07:26.880 --> 07:32.320]  makes it seem so much more awesome. Maybe just because it's much easier to use,\n",
            "[07:32.320 --> 07:35.600]  it's much easier to get what you want. You get it right more often the first time,\n",
            "[07:35.600 --> 07:39.680]  and ease of use matters a lot, even if the base capability was there before.\n",
            "[07:40.240 --> 07:47.840]  And like a feeling like it understood the question you were asking, or like it feels like you're\n",
            "[07:47.840 --> 07:51.840]  kind of on the same page. It's trying to help you. It's the feeling of alignment.\n",
            "[07:51.840 --> 07:56.480]  Yes. I mean, that could be a more technical term for it. And you're saying that not much\n",
            "[07:56.480 --> 07:59.600]  data is required for that, not much human supervision is required for that.\n",
            "[07:59.600 --> 08:07.040]  To be fair, we understand the science of this part at a much earlier stage than we do the science\n",
            "[08:07.040 --> 08:11.440]  of creating these large pre-trained models in the first place. But yes, less data, much less data.\n",
            "[08:11.440 --> 08:15.680]  That's so interesting. The science of human guidance.\n",
            "[08:18.000 --> 08:22.000]  That's a very interesting science. That's going to be a very important science to understand\n",
            "[08:22.720 --> 08:28.880]  how to make it usable, how to make it wise, how to make it ethical, how to make it aligned in\n",
            "[08:28.880 --> 08:31.040]  terms of all the kind of stuff we think about.\n",
            "[08:34.240 --> 08:37.840]  And it matters which are the humans and what is the process of incorporating that human\n",
            "[08:37.840 --> 08:42.240]  feedback. And what are you asking the humans? Is it two things? Are you asking them to rank things?\n",
            "[08:42.240 --> 08:48.160]  What aspects are you letting or asking the humans to focus in on? It's really fascinating.\n",
            "[08:48.160 --> 08:55.840]  But what is the data set it's trained on? Can you kind of loosely speak to the\n",
            "[08:55.920 --> 08:59.120]  enormity of this data set? The pre-training data set. The pre-training data set.\n",
            "[09:00.240 --> 09:03.760]  We spend a huge amount of effort pulling that together from many different sources.\n",
            "[09:04.480 --> 09:11.440]  There's like a lot of open source databases of information. We get stuff via partnerships.\n",
            "[09:11.440 --> 09:15.920]  There's things on the internet. A lot of our work is building a great data set.\n",
            "[09:17.040 --> 09:22.160]  How much of it is the memes subreddit? Not very much. Maybe it'd be more fun if it were more.\n",
            "[09:23.120 --> 09:28.560]  So some of it is Reddit. Some of it is news sources. There's a huge number of newspapers.\n",
            "[09:29.200 --> 09:33.840]  There's the general web. There's a lot of content in the world. More than I think most people think.\n",
            "[09:34.400 --> 09:41.680]  Yeah. There is too much where the task is not to find stuff but to filter out.\n",
            "[09:43.600 --> 09:47.680]  Is there a magic to that? There seems to be several components to solve.\n",
            "[09:48.320 --> 09:54.720]  The design of the algorithm. So like the architecture of the neural networks.\n",
            "[09:54.720 --> 09:58.000]  Maybe the size of the neural network. There's the selection of the data.\n",
            "[09:59.120 --> 10:05.360]  There's the human supervised aspect of it with RL with human feedback.\n",
            "[10:06.080 --> 10:10.880]  Yeah. I think one thing that is not that well understood about creation of this final product,\n",
            "[10:10.880 --> 10:15.360]  like what it takes to make GPT for the version of it we actually ship out and\n",
            "[10:15.360 --> 10:21.440]  that you get to use inside of chat GPT. The number of pieces that have to all come together\n",
            "[10:21.440 --> 10:25.840]  and then we have to figure out either new ideas or just executing existing ideas really well\n",
            "[10:26.240 --> 10:30.640]  at every stage of this pipeline. There's quite a lot that goes into it.\n",
            "[10:30.640 --> 10:36.560]  So there's a lot of problem solving. You've already said for GPT 4 in the blog post\n",
            "[10:36.560 --> 10:43.200]  and in general there's already a maturity that's happening on some of these steps.\n",
            "[10:43.200 --> 10:48.480]  Like being able to predict before doing the full training of how the model will behave.\n",
            "[10:48.480 --> 10:54.320]  Isn't that so remarkable by the way that there's a law of science that lets you predict for these\n",
            "[10:54.320 --> 10:58.960]  inputs. Here's what's going to come out the other end. Here's the level of intelligence you can\n",
            "[10:58.960 --> 11:05.760]  expect. Is it close to science or is it still because you said the word law in science\n",
            "[11:06.480 --> 11:11.600]  which are very ambitious terms close to it. I'd be accurate. Yes.\n",
            "[11:11.600 --> 11:15.600]  I'll say it's way more scientific than I ever would have dared to imagine.\n",
            "[11:15.600 --> 11:22.960]  So you can really know the peculiar characteristics of the fully trained system from just a little\n",
            "[11:22.960 --> 11:27.440]  bit of training. You know like any new branch of science there's we're going to discover new\n",
            "[11:27.440 --> 11:30.160]  things that don't fit the data and have to come up with better explanations and\n",
            "[11:30.800 --> 11:35.280]  you know that is the ongoing process of discovering science. But with what we know now\n",
            "[11:35.840 --> 11:41.040]  even when we had in that GPT 4 blog post like I think we should all just like be in awe of how\n",
            "[11:41.040 --> 11:44.320]  amazing it is that we can even predict to this current level.\n",
            "[11:44.320 --> 11:49.520]  Yeah. You can look at a one year old baby and predict how it's going to do on the SATs.\n",
            "[11:49.520 --> 11:56.080]  I don't know. Seemingly an equivalent one but because here we can actually in detail introspect\n",
            "[11:56.080 --> 12:01.520]  various aspects of the system you can predict. That said just to jump around you said\n",
            "[12:02.480 --> 12:07.280]  the language model that is GPT 4 it learns and quotes something.\n",
            "[12:08.000 --> 12:15.280]  In terms of science and art and so on is there within OpenAI within like folks like yourself\n",
            "[12:15.280 --> 12:21.200]  and Ilias iskiva and the engineers a deeper and deeper understanding of what that something is\n",
            "[12:22.160 --> 12:26.480]  or is it still a kind of beautiful magical mystery.\n",
            "[12:27.840 --> 12:33.040]  Well there's all these different emails that we could talk about and what's an eval.\n",
            "[12:33.120 --> 12:38.880]  Oh like how we measure a model as we're training it after we've trained it and say like you know\n",
            "[12:38.880 --> 12:42.960]  how good is this at some set of tasks. And also just on a small tangent thank you for sort of\n",
            "[12:42.960 --> 12:47.520]  opening sourcing the evaluation process. Yeah I think that'll be really helpful.\n",
            "[12:50.320 --> 12:56.720]  But the one that really matters is we pour all of this effort and money and time into this thing\n",
            "[12:57.280 --> 13:02.320]  and then what it comes out with like how useful is that to people. How much delight does that bring\n",
            "[13:02.320 --> 13:06.880]  people how much does that help them create a much better world new science new products new\n",
            "[13:06.880 --> 13:15.200]  services whatever. And that's the one that matters and understanding for a particular set of inputs\n",
            "[13:15.200 --> 13:20.560]  like how much value and utility to provide to people. I think we are understanding that better.\n",
            "[13:23.840 --> 13:28.400]  Do we understand everything about why the model does one thing and not one other thing?\n",
            "[13:28.400 --> 13:36.480]  Certainly not always but I would say we are pushing back like the fog of war more and more\n",
            "[13:36.480 --> 13:41.200]  and we are you know it took a lot of understanding to make GPT-4 for example.\n",
            "[13:41.760 --> 13:46.480]  But I'm not even sure we can ever fully understand like you said you would understand by asking\n",
            "[13:46.480 --> 13:52.800]  questions essentially because it's compressing all of the web like a huge sloth of the web\n",
            "[13:52.880 --> 14:00.080]  into a small number of parameters into one organized black box that is human wisdom.\n",
            "[14:01.120 --> 14:06.320]  What is that? Human knowledge let's say. Human knowledge. It's a good difference.\n",
            "[14:08.240 --> 14:12.160]  Is there a difference between knowledge? There's so there's facts and there's wisdom and I feel\n",
            "[14:12.160 --> 14:16.720]  like GPT-4 can be also full of wisdom. What's the leap from facts to wisdom?\n",
            "[14:16.720 --> 14:22.720]  You know a funny thing about the way we're training these models is I suspect too much\n",
            "[14:22.720 --> 14:29.680]  of the processing power for lack of a better word is going into using the models of database\n",
            "[14:29.680 --> 14:34.080]  instead of using the model as a reasoning engine. The thing that's really amazing about this\n",
            "[14:34.080 --> 14:38.400]  system is that for some definition of reasoning and we could of course quibble about it and there's\n",
            "[14:38.400 --> 14:43.600]  plenty for which definitions this wouldn't be accurate but for some definition it can do some\n",
            "[14:43.600 --> 14:48.480]  kind of reasoning and you know maybe like the scholars and the experts and like the armchair\n",
            "[14:48.480 --> 14:53.280]  quarterbacks on Twitter would say no it can't you're misusing the word you know whatever whatever\n",
            "[14:53.280 --> 14:58.240]  but I think most people who have used this system would say okay it's doing something in this direction\n",
            "[14:58.960 --> 15:08.080]  and I think that's remarkable and the thing that's most exciting and somehow out of\n",
            "[15:09.680 --> 15:16.080]  ingesting human knowledge it's coming up with this reasoning and capability however we're going\n",
            "[15:16.160 --> 15:23.600]  to talk about that. Now in some senses I think that will be additive to human wisdom and in some\n",
            "[15:23.600 --> 15:28.240]  other senses you can use GPT-4 for all kinds of things and say that appears that there's no wisdom\n",
            "[15:28.240 --> 15:34.320]  in here whatsoever. Yeah at least in interaction with humans it seems to possess wisdom especially\n",
            "[15:34.320 --> 15:41.440]  when there's a continuous interaction of multiple problems so I think what on the Chad GPT site it says\n",
            "[15:42.080 --> 15:50.240]  the dialogue format makes it possible for Chad GPT to answer follow-up questions admit its mistakes\n",
            "[15:50.240 --> 15:56.000]  challenge incorrect premises and reject inappropriate requests but also there's a feeling like it's\n",
            "[15:56.000 --> 16:01.760]  struggling with ideas. Yeah it's always tempting to anthropomorphize this stuff too much but I also\n",
            "[16:01.760 --> 16:08.080]  feel that way. Maybe I'll take a small tangent towards Jordan Peterson who posted on Twitter\n",
            "[16:09.040 --> 16:15.600]  this kind of political question everyone has a different question than when I asked Chad GPT first\n",
            "[16:15.600 --> 16:21.680]  right like the different directions you want to try the dark thing. It somehow says a lot about\n",
            "[16:21.680 --> 16:28.480]  people the first thing. Oh no oh no we don't we don't have to reveal what I asked. We do not.\n",
            "[16:29.280 --> 16:37.120]  I of course ask mathematical questions I've never asked anything dark but Jordan asked it to say\n",
            "[16:37.200 --> 16:43.280]  positive things about the current president Joe Biden and previous president Donald Trump and then\n",
            "[16:44.880 --> 16:52.240]  he asked GPT as a follow-up to say how many characters how long is the string that you generated and he\n",
            "[16:52.240 --> 16:58.240]  showed that the response that contained positive things about Biden was much longer or longer than\n",
            "[16:59.120 --> 17:04.880]  that about Trump and Jordan asked the system to can you rewrite it with an equal number\n",
            "[17:04.880 --> 17:10.560]  equal length string which all of this is just remarkable to me that it understood but it failed\n",
            "[17:10.560 --> 17:22.240]  to do it and it was interested in GPT Chad GPT I think that was 3.5 based was kind of introspective\n",
            "[17:22.240 --> 17:31.760]  about yeah it seems like I failed to do the job correctly and Jordan framed it as Chad GPT was\n",
            "[17:31.760 --> 17:39.280]  lying and aware that it's lying but that framing that's a human anthropomorphization I think\n",
            "[17:40.720 --> 17:47.200]  but that that kind of yeah there there seemed to be a struggle within GPT to understand\n",
            "[17:50.080 --> 17:55.840]  how to do like what it means to generate a text of the same length\n",
            "[17:56.800 --> 18:04.560]  in an answer to a question and also in a sequence of prompts how to understand that it failed to do\n",
            "[18:04.560 --> 18:10.880]  so previously and where it succeeded and all of those like multi like parallel reasonings that\n",
            "[18:10.880 --> 18:16.640]  is doing it just seems like it's struggling so two separate things going on here number one some\n",
            "[18:16.640 --> 18:22.000]  of the things that seem like they should be obvious and easy these models really struggle with yeah\n",
            "[18:22.000 --> 18:25.840]  so I haven't seen this particular example but counting characters counting words that sort of\n",
            "[18:25.840 --> 18:31.120]  stuff that is hard for these models to do well the way they're architected that won't be very accurate\n",
            "[18:32.160 --> 18:38.160]  second we are building in public and we are putting out technology because we think it is\n",
            "[18:38.160 --> 18:43.040]  important for the world to get access to this early to shape the way it's going to be developed to\n",
            "[18:43.040 --> 18:47.360]  help us find the good things and the bad things and every time we put out a new model and we've\n",
            "[18:47.440 --> 18:52.240]  just really felt this with GPT for this week the collective intelligence and ability of the\n",
            "[18:52.240 --> 18:58.000]  outside world helps us discover things we cannot imagine we could have never done internally and\n",
            "[18:58.560 --> 19:03.120]  both like great things that the model can do new capabilities and real weaknesses we have to fix\n",
            "[19:03.120 --> 19:10.160]  and so this iterative process of putting things out finding the the the great parts the bad parts\n",
            "[19:10.160 --> 19:15.920]  improving them quickly and giving people time to feel the technology and shape it with us\n",
            "[19:15.920 --> 19:21.360]  and provide feedback we believe is really important the trade-off of that is the trade-off of building\n",
            "[19:21.360 --> 19:25.680]  in public which is we put out things that are going to be deeply imperfect we want to make our\n",
            "[19:25.680 --> 19:32.640]  mistakes while the stakes are low we want to get it better and better each rep but the like the\n",
            "[19:32.640 --> 19:38.240]  bias of chat GPT when it launched with 3.5 was not something that I certainly felt proud of\n",
            "[19:39.040 --> 19:43.040]  it's gotten much better with GPT for many of the critics and I really respect this have said hey\n",
            "[19:43.040 --> 19:49.360]  a lot of the problems that I had with 3.5 are much better in four but also no two people are ever\n",
            "[19:49.360 --> 19:55.040]  going to agree that one single model is unbiased on every topic and I think the answer there is\n",
            "[19:55.040 --> 20:02.000]  just going to be to give users more personalized control granular control over time and I should\n",
            "[20:02.000 --> 20:09.840]  say in this point yeah I've gotten to know Jordan Peterson and I tried to talk to GPT for about\n",
            "[20:09.920 --> 20:17.840]  Jordan Peterson and I asked it if Jordan Peterson is the fascist first of all it gave context\n",
            "[20:17.840 --> 20:22.720]  it described actual like description of who Jordan Peterson is his career psychologist and so on\n",
            "[20:23.360 --> 20:32.640]  it stated that some number of people have called Jordan Peterson the fascist but there is no\n",
            "[20:32.640 --> 20:38.480]  factual grounding to those claims and it described a bunch of stuff that Jordan believes like he's\n",
            "[20:38.480 --> 20:46.800]  been an outspoken critic of various totalitarian ideologies and he believes in\n",
            "[20:49.520 --> 20:58.800]  individualism and various freedoms that contradict the ideology of fascism and so on and it goes on\n",
            "[20:58.800 --> 21:04.400]  and on like really nicely and it wraps it up it's like a it's a college essay I was like damn one thing\n",
            "[21:04.400 --> 21:10.640]  that I hope these models can do is bring some nuance back to the world yes it felt it felt\n",
            "[21:10.640 --> 21:15.600]  really nuanced you know Twitter kind of destroyed some and maybe we can get some back now that really\n",
            "[21:15.600 --> 21:24.400]  is exciting like for example I asked of course you know did did the covid virus leak from a lab\n",
            "[21:24.400 --> 21:31.760]  again answer very nuanced there's two hypotheses it like describe them it described the the amount\n",
            "[21:31.760 --> 21:37.440]  of data that's available for each it was like it was like a breath of fresh hair when I was a\n",
            "[21:37.440 --> 21:41.200]  little kid I thought building AI we didn't really call it AGI at the time I thought building\n",
            "[21:41.200 --> 21:44.880]  a happy like the coolest thing ever I never really thought I would get the chance to work on it\n",
            "[21:44.880 --> 21:49.360]  but if you had told me that not only I would get the chance to work on it but that after making\n",
            "[21:49.360 --> 21:56.640]  like a very very larval proto AGI thing that the thing I'd have to spend my time on is you know\n",
            "[21:56.640 --> 22:00.640]  trying to like argue with people about whether the number of characters it said nice things about\n",
            "[22:00.640 --> 22:04.880]  one person was different than the number of characters it said nice about some other person\n",
            "[22:04.880 --> 22:08.800]  if you hand people an AGI and that's what they want to do I wouldn't have believed you but I\n",
            "[22:08.800 --> 22:14.560]  understand him one now and I do have empathy for it so what you're implying in that statement is\n",
            "[22:14.560 --> 22:19.280]  we took such giant leaps and the big stuff that they were complaining or arguing about small stuff\n",
            "[22:19.280 --> 22:22.800]  well the small stuff is the big stuff in aggregate so I get it it's just like I\n",
            "[22:24.640 --> 22:30.560]  and I also like I get why this is such an important issue this is a really important issue\n",
            "[22:31.120 --> 22:37.920]  but that somehow we like somehow this is the thing that we get caught up in versus like\n",
            "[22:38.560 --> 22:43.680]  what is this going to mean for our future now maybe you say this is critical to what this is\n",
            "[22:43.680 --> 22:47.280]  going to mean for our future the thing that it says more characters about this person than this\n",
            "[22:47.280 --> 22:51.760]  person and who's deciding that and how it's being decided and how the users get control over that\n",
            "[22:52.480 --> 22:56.720]  maybe that is the most important issue but I wouldn't have guessed it at the time when I was\n",
            "[22:56.720 --> 23:06.160]  like eight-year-old yeah I mean there is and you do there's folks at open AI including yourself\n",
            "[23:06.160 --> 23:11.440]  that do see the importance of these issues to discuss about them under the big banner of AI\n",
            "[23:11.440 --> 23:17.040]  safety that's something that's not often talked about with the release of GPT for how much went into\n",
            "[23:17.040 --> 23:23.280]  the safety concerns how long also you spend on the safety concerns can you can you go through\n",
            "[23:23.280 --> 23:28.800]  some of that process yeah sure what went into AI safety considerations of GPT for release\n",
            "[23:29.360 --> 23:36.960]  so we finished last summer we immediately started giving it to people to to red team\n",
            "[23:37.920 --> 23:42.720]  we started doing a bunch of our own internal safety emails on it we started trying to work\n",
            "[23:42.720 --> 23:50.560]  on different ways to align it and that combination of an internal and external effort plus building\n",
            "[23:50.640 --> 23:55.440]  a whole bunch of new ways to align the model and we didn't get it perfect by far but one thing\n",
            "[23:55.440 --> 24:01.440]  that I care about is that our degree of alignment increases faster than our rate of capability\n",
            "[24:01.440 --> 24:07.760]  progress and that I think will become more and more important over time and I know I think we made\n",
            "[24:07.760 --> 24:12.160]  reasonable progress there to a to a more aligned system than we've ever had before I think this is\n",
            "[24:12.880 --> 24:18.160]  the most capable and most aligned model that we've put out we were able to do a lot of testing on\n",
            "[24:18.160 --> 24:24.720]  it and that takes a while and I totally get why people were like give us GPT for right away\n",
            "[24:26.080 --> 24:31.680]  but I'm happy we did it this way is there some wisdom some insights about that process\n",
            "[24:31.680 --> 24:37.600]  you learned like how to how to solve that problem you can speak to how to solve the like the alignment\n",
            "[24:37.600 --> 24:43.600]  problem so I want to be very clear I do not think we have yet discovered a way to align a super\n",
            "[24:43.600 --> 24:48.480]  powerful system we have we have something that works for our current skill called our LHF\n",
            "[24:49.680 --> 24:57.280]  and we can talk a lot about the benefits of that and the utility it provides it's not just an\n",
            "[24:57.280 --> 25:02.960]  alignment maybe it's not even mostly an alignment capability it helps make a better system a more\n",
            "[25:02.960 --> 25:09.520]  usable system and this is actually something that I don't think people outside the field understand\n",
            "[25:09.520 --> 25:16.000]  enough it's easy to talk about alignment and capability as orthogonal vectors they're very close\n",
            "[25:17.280 --> 25:23.440]  better alignment techniques lead to better capabilities and vice versa there's cases that are different\n",
            "[25:23.440 --> 25:28.960]  and they're important cases but on the whole I think things that you could say like our LHF\n",
            "[25:28.960 --> 25:33.600]  or interpretability that sound like alignment issues also help you make much more capable models\n",
            "[25:34.240 --> 25:40.640]  and the division is just much fuzzier than people think and so in some sense the work we do to\n",
            "[25:40.640 --> 25:45.920]  make GPD4 safer and more aligned looks very similar to all the other work we do of solving\n",
            "[25:45.920 --> 25:54.320]  the research and engineering problems associated with creating useful and powerful models so our\n",
            "[25:54.320 --> 26:00.640]  LHF is the process that came applied very broadly across the entire system where human\n",
            "[26:00.640 --> 26:08.720]  basically votes what's the better way to say something what's you know if a person asks do I\n",
            "[26:08.720 --> 26:15.680]  look fat in this dress there's different ways to ask that question that's aligned with human\n",
            "[26:15.680 --> 26:21.040]  civilization and there's no one set of human values or there's no one set of right answers\n",
            "[26:21.040 --> 26:28.160]  to human civilization so I think what's going to have to happen is we will need to agree on as a\n",
            "[26:28.240 --> 26:33.120]  society on very broad bounds we'll only be able to agree on a very broad bounds of what these\n",
            "[26:33.120 --> 26:38.720]  systems can do and then within those maybe different countries have different RLHF tunes\n",
            "[26:38.720 --> 26:43.760]  certainly individual users have very different preferences we launched this thing with GPD4\n",
            "[26:43.760 --> 26:50.800]  called the system message which is not RLHF but is a way to let users have a good degree of\n",
            "[26:51.600 --> 26:57.920]  steerability over what they want and I think things like that will be important he describes\n",
            "[26:57.920 --> 27:02.640]  as the message and in general how you were able to make GPD4 more steerable\n",
            "[27:05.120 --> 27:09.440]  based on the interaction that the user can have with it which is one of his big really powerful\n",
            "[27:09.440 --> 27:16.720]  things so this system message is a way to say uh you know hey model please pretend like you or\n",
            "[27:16.720 --> 27:25.520]  please only answer this message as if you were Shakespeare doing thing X or please only respond\n",
            "[27:25.520 --> 27:30.560]  with JSON no matter what was one of the examples from our blog post but you could also say any\n",
            "[27:30.560 --> 27:39.840]  number of other things to that and then we we we tuned GPD4 in a way to really treat the system\n",
            "[27:39.840 --> 27:44.880]  message with a lot of authority I'm sure there's jail they'll always not always hopefully but for\n",
            "[27:44.880 --> 27:49.280]  a long time there'll be more jail breaks and we'll keep sort of learning about those but we\n",
            "[27:49.280 --> 27:54.080]  program we develop whatever you want to call it the model in such a way to learn that it's\n",
            "[27:54.080 --> 27:59.280]  supposed to really use that system message can you speak to the kind of the process of writing\n",
            "[27:59.280 --> 28:06.240]  in designing a great prompt as you steer GPD4 I'm not good at this I've met people who are yeah and\n",
            "[28:06.880 --> 28:13.120]  the creativity the kind of they almost some of them almost treated like debugging software\n",
            "[28:13.920 --> 28:21.120]  um but also they they I met people who spend like you know 12 hours a day for a month on end at\n",
            "[28:21.120 --> 28:26.480]  on this and they really get a feel for the model and a feel how different parts of a\n",
            "[28:27.440 --> 28:33.600]  prompt compose with each other like literally the ordering of words this yeah where you put the\n",
            "[28:33.600 --> 28:39.360]  clause when you modify something what kind of word to do it with yeah it's so fascinating because\n",
            "[28:39.440 --> 28:44.080]  like it's remarkable in some sense that's what we do with human conversation right and interacting\n",
            "[28:44.080 --> 28:51.440]  with humans we try to figure out like what words to use to unlock greater wisdom from the other\n",
            "[28:52.480 --> 28:58.240]  the other party the friends of yours are significant others here you get to try it over and over and\n",
            "[28:58.240 --> 29:02.560]  over and over unlimited you could experiment yeah there's all these ways that the kind of\n",
            "[29:02.560 --> 29:08.640]  analogies from humans to a eyes like breakdown and the parallelism the sort of unlimited rollouts\n",
            "[29:08.720 --> 29:14.720]  that's a big one yeah yeah but there's still some parallels that don't break down that there is\n",
            "[29:14.720 --> 29:19.440]  some hundred people here because it's trained on human data there's um it feels like it's a way\n",
            "[29:19.440 --> 29:25.680]  to learn about ourselves by interacting with it some of it as the smarter and smarter gets the\n",
            "[29:25.680 --> 29:33.040]  more represents the more it feels like another human in terms of um the kind of way you would\n",
            "[29:33.040 --> 29:38.800]  phrase the prompt to get the kind of thing you want back and that's interesting because that\n",
            "[29:38.800 --> 29:44.160]  is the art form as you collaborate with it as an assistant this becomes more relevant for\n",
            "[29:44.960 --> 29:48.240]  this is relevant everywhere but it's also very relevant for programming for example\n",
            "[29:48.880 --> 29:55.040]  um I mean just on that topic how do you think gpt4 and all the advancements with gpt change\n",
            "[29:55.040 --> 30:01.040]  the nature of programming today's monday we launched the previous tuesday so it's six days\n",
            "[30:01.520 --> 30:06.160]  the degree while the degree to which it has already changed programming\n",
            "[30:07.920 --> 30:14.560]  and what I have observed from how my friends are creating the tools that are being built on top of\n",
            "[30:14.560 --> 30:23.520]  it um I think this is where we'll see some of the most impact in the short term it's amazing what\n",
            "[30:23.520 --> 30:30.880]  people are doing it's amazing how this tool the leverage it's giving people to do their job\n",
            "[30:30.880 --> 30:36.960]  or their creative work better and better and better it's it's super cool so in the process\n",
            "[30:37.680 --> 30:44.880]  the iterative process you could um ask it to generate a code to do something and then\n",
            "[30:46.720 --> 30:51.520]  the something the code it generates and the something that the code does if you don't like it you can\n",
            "[30:51.520 --> 30:57.200]  ask it to adjust it it's like it's a it's a weird it's a different kind of way of debugging I guess\n",
            "[30:57.200 --> 31:01.040]  for sure the first versions of these systems were sort of you know one shot you sort of you\n",
            "[31:01.040 --> 31:05.440]  said what you wanted it wrote some code and that was it uh now you can have this back and forth\n",
            "[31:05.440 --> 31:09.680]  dialogue where you can say no no I meant this or no no fix this bug or no no do this and then of\n",
            "[31:09.680 --> 31:14.800]  course the next version is the system can debug more on its own and kind of try to like catch\n",
            "[31:14.800 --> 31:23.440]  mistakes as it's making them but this idea of dialogue interfaces and iterating with the computer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c111ce4b8b4e>\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"### {video_path_local}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     video_transcription = whisper.transcribe(\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mwhisper_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mdecode_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_reset_since\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDecodingResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mdecode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mdecode_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mneeds_fallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# call the main sampling loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_speech_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36m_main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mlogits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup_caching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mkv_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     ):\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# otherwise, perform key/value projections for self- or cross-attention as usual.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxa\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/model.py\u001b[0m in \u001b[0;36msave_to_cache\u001b[0;34m(module, _, output)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}